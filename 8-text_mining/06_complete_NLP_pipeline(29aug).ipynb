{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0604b31e-acc4-40f6-958c-255d4ec29d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bfbe3a6-81f2-42a7-a285-e6d9329b677d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple | to | build | a | Hong | Kong | factory | for | $ | 6 | million | \n",
      "------\n",
      "Apple - ORG - Companies, agencies, institutions, etc.\n",
      "Hong Kong - GPE - Countries, cities, states\n",
      "$6 million - MONEY - Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc8=nlp(u'Apple to build a Hong Kong factory for $6 million')\n",
    "\n",
    "for token in doc8:\n",
    "    print(token.text,end=' | ')#here we will be separating the each tokens by \"|\"\n",
    "\n",
    "print('\\n------')\n",
    "\n",
    "for ent in doc8.ents:\n",
    "    print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f096a800-9c12-420a-a00e-d9aac2d577c9",
   "metadata": {},
   "source": [
    "Noun Chunks to Doc.ents .DOc.noun_chunks are another object property.NOun CHUNKS are \"base noun phrases\" -flat phrases that have noun as their head.You can think of noun chunk as noun plus...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f56a4c19-06b2-40fc-896b-3b62cbde7941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars\n",
      "insurance liability\n",
      "manufacturers\n"
     ]
    }
   ],
   "source": [
    "doc9=nlp(u\"Autonomous cars shift insurance liability towards manufacturers.\")\n",
    "for chunk in doc9.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9ab83b7-e13f-4c4b-af93-869285f1431a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red cars\n",
      "higher insurance rates\n"
     ]
    }
   ],
   "source": [
    "doc10=nlp(u\"Red cars do not carry higher insurance rates.\")\n",
    "\n",
    "for chunk in doc10.noun_chunks:#chunking or groupin gthe nouns like red cars\n",
    "\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e410f0-bc86-4ca8-a106-26cc054d6bc0",
   "metadata": {},
   "source": [
    "stemming is a somewhat crude method for cateloging related words;it essentially chops off letters from the end until the stem is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653a4dbf-5f09-47db-a4ea-e73561d60a4e",
   "metadata": {},
   "source": [
    "# Porter Stemmer\n",
    "the algorithm employs the 5 phases of word reduction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f41508ea-bb89-4229-aa39-46896fe4b240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the toolkit and the full porter stemmer ibrAary\n",
    "import nltk\n",
    "from nltk.stem.porter import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ead9c528-8a6b-428c-9fbe-ccc3557a8b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48c0d948-d949-4bc1-8ea0-0e05a0559c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=['run','runner','running','ran','easily','fairly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b32e262-7023-4278-9172-3eb1eb6e3c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run-->run\n",
      "runner-->runner\n",
      "running-->run\n",
      "ran-->ran\n",
      "easily-->easili\n",
      "fairly-->fairli\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+'-->'+p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1320805d-b07a-48f4-91fd-020ce839b5aa",
   "metadata": {},
   "source": [
    "### NOte:\n",
    "the stemmer is recognizes the \"runner\" as a noun....phn ss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a3abf7-3a6a-4c2e-8e8f-c18be2c45961",
   "metadata": {},
   "source": [
    "# Snowballstemmer\n",
    "It offers a slight improvement over the original Porter stemmer ,both in logic ans speed .Since nltk uses the name Snowballstemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17aabd1f-6463-4e08-a53a-426c9d642ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "#The Snowball Stemmmer requires tht you can pass a language parameter\n",
    "s_stemmer=SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5dc3420a-a550-4261-bc10-937865c0cb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=['run','runner','running','ran','runs','easily','fairly']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b7bf71a-ac74-4636-a2a4-21cd0b3bc593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run-->run\n",
      "runner-->runner\n",
      "running-->run\n",
      "ran-->ran\n",
      "runs-->run\n",
      "easily-->easili\n",
      "fairly-->fair\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+'-->'+s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bb80f3-ab58-4ce2-a0e0-74efcf3faf5a",
   "metadata": {},
   "source": [
    "Stemming has its drawbacks .If given the token saw,stemming might always return saw ,whereas lemmatization would likely return either see or saw depending on the whether the use of token was as a verb or as a noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8e20685-59f3-46a1-92bc-4e5ca808a72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I-->i\n",
      "am-->am\n",
      "meeting-->meet\n",
      "him-->him\n",
      "tomorrow-->tomorrow\n",
      "at-->at\n",
      "the-->the\n",
      "meeting-->meet\n"
     ]
    }
   ],
   "source": [
    "phrase='I am meeting him tomorrow at the meeting'\n",
    "for word in phrase.split():\n",
    "    print(word+'-->'+p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ba94b8-9583-4e9a-aa58-c7e1cd84f645",
   "metadata": {},
   "source": [
    "here the word meeting appears twice one in the form of verb and one as a noun\n",
    "\n",
    "Lemmatization in contrast of to stemming,it looks beyond word reduction and morphological analysis of words.The lemma of \"was \" is \"be\" and \"mice\" is \"mouse\".m\"meeting\" might be \"meet\" or \"meeting\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35cef640-4dda-4cc0-b0d4-631fb32d94dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform standard imports:\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f9c6203-9690-4769-8d0c-5952d484fbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t PRON \t 4690420944186131903 \t I\n",
      "am \t AUX \t 10382539506755952630 \t be\n",
      "a \t DET \t 11901859001352538922 \t a\n",
      "runner \t NOUN \t 12640964157389618806 \t runner\n",
      "in \t ADP \t 3002984154512732771 \t in\n",
      "a \t DET \t 11901859001352538922 \t a\n",
      "race \t NOUN \t 8048469955494714898 \t race\n",
      "because \t SCONJ \t 16950148841647037698 \t because\n",
      "I \t PRON \t 4690420944186131903 \t I\n",
      "love \t VERB \t 3702023516439754181 \t love\n",
      "to \t PART \t 3791531372978436496 \t to\n",
      "run \t VERB \t 12767647472892411841 \t run\n",
      "since \t SCONJ \t 10066841407251338481 \t since\n",
      "I \t PRON \t 4690420944186131903 \t I\n",
      "ran \t VERB \t 12767647472892411841 \t run\n",
      "today \t NOUN \t 11042482332948150395 \t today\n"
     ]
    }
   ],
   "source": [
    "doc1=nlp(u\"I am a runner in a race because I love to run since I ran today\")\n",
    "\n",
    "for token in doc1:\n",
    "    print(token.text,'\\t',token.pos_,'\\t',token.lemma,'\\t',token.lemma_)\n",
    "    #token.text will tmake the tokens \n",
    "   # token.pos_ - will give the parts of speech\n",
    "#token lemma-number which is defined to them\n",
    "#tokken.lemma_ defines the lemmaization words "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d476c0b0-e5e0-493c-8e5d-d14240ebc94a",
   "metadata": {},
   "source": [
    "#function to display lemma since the display above is staggered and hard to read lets write a function that displays the information we want more neatly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "817f8234-bb28-4f28-b68e-76eed96847ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_lemmas(text):\n",
    "    for token in text:\n",
    "        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d3f57e7-1808-4462-b52b-ae61ca1189f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we are using an f-string to format the printed text by setting the minimum field widths and adding a left align to the lemma hash value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dbb8bd60-ceec-4746-8552-4b42768d73dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I            PRON   4690420944186131903    I\n",
      "saw          VERB   11925638236994514241   see\n",
      "eighteen     NUM    9609336664675087640    eighteen\n",
      "mice         NOUN   1384165645700560590    mouse\n",
      "today        NOUN   11042482332948150395   today\n",
      "I \t PRON \t 4690420944186131903 \t I\n",
      "am \t AUX \t 10382539506755952630 \t be\n",
      "a \t DET \t 11901859001352538922 \t a\n",
      "runner \t NOUN \t 12640964157389618806 \t runner\n",
      "in \t ADP \t 3002984154512732771 \t in\n",
      "a \t DET \t 11901859001352538922 \t a\n",
      "race \t NOUN \t 8048469955494714898 \t race\n",
      "because \t SCONJ \t 16950148841647037698 \t because\n",
      "I \t PRON \t 4690420944186131903 \t I\n",
      "love \t VERB \t 3702023516439754181 \t love\n",
      "to \t PART \t 3791531372978436496 \t to\n",
      "run \t VERB \t 12767647472892411841 \t run\n",
      "since \t SCONJ \t 10066841407251338481 \t since\n",
      "I \t PRON \t 4690420944186131903 \t I\n",
      "ran \t VERB \t 12767647472892411841 \t run\n",
      "today \t NOUN \t 11042482332948150395 \t today\n"
     ]
    }
   ],
   "source": [
    "doc2=nlp(u\"I saw eighteen mice today\")\n",
    "show_lemmas(doc2)\n",
    "\n",
    "for token in doc1:\n",
    "    print(token.text,'\\t',token.pos_,'\\t',token.lemma,'\\t',token.lemma_)\n",
    "    #token.text will tmake the tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "81126ce7-2723-4ed8-b730-533200f786ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I            PRON   4690420944186131903    I\n",
      "am           AUX    10382539506755952630   be\n",
      "meeting      VERB   6880656908171229526    meet\n",
      "him          PRON   1655312771067108281    he\n",
      "tomorrow     NOUN   3573583789758258062    tomorrow\n",
      "at           ADP    11667289587015813222   at\n",
      "the          DET    7425985699627899538    the\n",
      "meeting      NOUN   14798207169164081740   meeting\n"
     ]
    }
   ],
   "source": [
    "doc3=nlp(u\"I am meeting him tomorrow at the meeting\")\n",
    "show_lemmas(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0794c362-e2d3-4c1a-942f-9c72d95b65de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That         PRON   4380130941430378203    that\n",
      "'s           AUX    10382539506755952630   be\n",
      "an           DET    15099054000809333061   an\n",
      "enormous     ADJ    17917224542039855524   enormous\n",
      "automobile   NOUN   7211811266693931283    automobile\n"
     ]
    }
   ],
   "source": [
    "doc4=nlp(u\"That's an enormous automobile\")\n",
    "show_lemmas(doc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eabbf63a-fbb4-4261-9df2-77306f774e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopwords like \"a\" and \"the \" appear so frequently thst the y dont require tagging as thoroughly as nouns ...not necessary to be there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bfe733da-1801-42d0-9b5f-3a0ef83cb56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform standard imports:\n",
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d357fdb7-f729-4041-91e5-e7fb3dc70b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'while', 'if', 'after', 'whence', 'when', 'anyway', 'are', 'within', 'two', 'herein', 'nine', 'us', 'part', 'for', 'whether', 'somehow', 'thereafter', 'sixty', 'top', 'using', 'therein', 'somewhere', 'into', '’m', 'it', 'else', 'thereupon', 'i', 'due', 'third', 'moreover', 'has', 'serious', \"'d\", 'yourselves', 'himself', 'besides', 'five', 'see', 'hereupon', 'namely', 'becoming', 'the', 'until', 'give', 'amongst', 'behind', 'either', \"'re\", 'thereby', 'thru', 'ca', 'hers', 'too', \"n't\", 'hundred', 'then', 'whither', 'to', 'by', 'used', 'under', '‘s', 'hereafter', '‘ll', 'ever', 'nevertheless', 'ourselves', 'may', 'cannot', 'nowhere', 'be', 'before', '’ve', 'back', '‘re', 'they', 'whereupon', 'done', 'have', 'sometimes', 'nobody', 'between', 'others', 'except', 'again', 'nothing', 'me', 'whose', 'go', 'least', '‘d', 'onto', 'n’t', 'whenever', 'because', 'same', 'towards', 'am', 'now', 'here', 'every', 'yet', 'anyone', 'them', 'beside', 'anyhow', 'three', 'mine', 'these', 'none', 'during', 'ours', 'among', 'move', 'along', 'against', 'thus', 'meanwhile', 'off', '’s', 'became', 'however', 'less', 'everything', 'of', 'everyone', 'rather', 'fifty', 'keep', 'although', 'is', 'per', 'next', 'whereby', 'might', 'most', 'there', 'nor', 'get', 'also', \"'ve\", 'we', 'whereas', 'wherever', 'some', 'regarding', \"'m\", 'whereafter', 'seemed', '‘m', 'his', 'was', 'becomes', 'elsewhere', 'otherwise', \"'ll\", 'few', '’ll', 'often', 'n‘t', 'that', 'more', 'please', 'never', 'everywhere', 'above', 'quite', 'yours', 'former', 'whom', 'any', 'he', 'take', 'side', 'what', 'can', 'last', 'should', 'six', 'further', 'who', 'many', 'whatever', 'anywhere', '’d', 'something', 'at', 'forty', 'neither', 'myself', 'doing', 'ten', 'will', 'could', 'already', 'very', 'noone', 'than', 'once', 'seem', 'she', 'first', 'whoever', 'over', 'own', 'front', 'so', 'being', 'afterwards', 'him', 'without', 'only', 'had', 'enough', 'did', 'become', 'twelve', 'anything', 'eleven', 'herself', 'really', 'through', 'would', 'just', 'name', 'whole', 'how', 'via', 'do', 'with', 'why', 'several', 'been', 'in', 'our', 'empty', 'therefore', 'my', '’re', 'latterly', 'alone', 'all', 'across', 'as', 'but', '‘ve', 'seems', 'make', 'its', 'yourself', 'itself', 'you', 'another', 'about', 'around', 'wherein', 'or', 'were', 'your', 'hereby', 'each', 'no', 'show', 'beyond', 'though', 'which', 'down', 'throughout', 'out', 'together', 'where', 'a', 'an', 'toward', 'mostly', 'bottom', 'made', 'indeed', 'formerly', 'amount', 'well', 'latter', 'twenty', 'sometime', 'full', 're', 'someone', 'various', 'say', 'and', 'up', 'much', 'both', 'one', 'thence', 'put', 'four', 'fifteen', 'from', 'other', 'since', 'their', 'still', 'does', 'almost', 'even', 'not', \"'s\", 'unless', 'themselves', 'seeming', 'those', 'always', 'hence', 'perhaps', 'must', 'on', 'eight', 'such', 'upon', 'call', 'her', 'below', 'this', 'beforehand'}\n"
     ]
    }
   ],
   "source": [
    "#print the set of spacyy  default stop words\n",
    "print(nlp.Defaults.stop_words)\n",
    "#326 stop words are there in english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0693129e-b18b-46ab-a2b5-4c59a5d64417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ba5cf5-7040-442e-897a-8af47542b840",
   "metadata": {},
   "source": [
    "# To see a word is a stop word or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5c06b70d-742e-4614-98d0-69f0e47c40be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['myself'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "479cb702-2e10-42c1-a4c9-597753ea5945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['mystery'].is_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cdc048-e6fa-4482-bc88-75cd83fd6273",
   "metadata": {},
   "source": [
    "# To add a stopword\n",
    "there may be mobile words like btw (by the way) should be considered as stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7581d94d-53cb-46a1-82bb-c7fc16ebabaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the word to the set of stop words.Use lowercase!\n",
    "nlp.Defaults.stop_words.add('btw')\n",
    "\n",
    "#set the stop_word tag on the lexeme\n",
    "nlp.vocab['btw'].is_stop=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "23c0fc7d-621d-4c05-92dc-3ba51c2c65d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d72a1e1b-9a52-4b3b-b955-ef84b0a2d147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['btw'].is_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3152d8b-7637-4b4a-bdce-feb09bd932da",
   "metadata": {},
   "source": [
    "# to remove a stopsword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4712fbff-478a-4579-8fde-4abc0ee44c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the word from the set of stop words\n",
    "nlp.Defaults.stop_words.remove('beyond')\n",
    "\n",
    "#remove the stop_words tag from the lexeme\n",
    "nlp.vocab['beyond'].is_stop=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bdcc8435-4122-4490-99d1-c6a108d32d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "14f36968-fc69-404c-aed5-815393de083c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['beyond'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f831b53-603d-4b73-b092-553905b4328b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
